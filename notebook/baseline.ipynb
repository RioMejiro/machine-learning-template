{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "8d1c13f8-864f-45e5-b90c-76024bd4e0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os, glob\n",
    "import joblib\n",
    "import random\n",
    "import math\n",
    "from tqdm import tqdm \n",
    "from collections import OrderedDict\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy import signal\n",
    "from scipy.signal import argrelmax\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from math import pi, sqrt, exp\n",
    "import sklearn,sklearn.model_selection\n",
    "import torch\n",
    "from torch import nn,Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n",
    "from torch.optim import AdamW\n",
    "# import pytorch_lightning as pl\n",
    "# from pytorch_lightning import seed_everything\n",
    "import lightning as L\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import (\n",
    "    LearningRateMonitor,\n",
    "    ModelCheckpoint,\n",
    "    RichModelSummary,\n",
    "    RichProgressBar,\n",
    ")\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "import timm\n",
    "from timm.scheduler import CosineLRScheduler\n",
    "\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from torchvision.transforms.functional import resize\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "from pyarrow.parquet import ParquetFile\n",
    "import pyarrow as pa \n",
    "import ctypes\n",
    "import polars as pl\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70b40db5-e322-41da-b2b3-f28eb536b043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/project\")\n",
    "from nn_utils import loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "87343e61-fa8b-4c79-8d9b-15473f8ed10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Seed set to 86\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CFG:\n",
    "    # Fundamental config\n",
    "    NOTDEBUG = True # False -> DEBUG, True -> normally train\n",
    "    WORKERS = os.cpu_count() // 2\n",
    "    N_FOLDS = 5\n",
    "    TRAIN_FOLD = 0\n",
    "    #一時間720step\n",
    "    MAX_LEN = 12*720\n",
    "    USE_AMP = False\n",
    "    SEED = 86\n",
    "\n",
    "    # Model config\n",
    "    HIDDEN = 256 if NOTDEBUG else 16\n",
    "    EMB_DIM = 16\n",
    "    KS = 31 if NOTDEBUG else 7\n",
    "    N_BLKS = 5 if NOTDEBUG else 2\n",
    "    DROPOUT = 0.2\n",
    "\n",
    "    # Optimizer config\n",
    "    LR = 5e-4\n",
    "    WD = 1e-2\n",
    "    WARMUP_PROP = 0.1\n",
    "    # LR_INIT = 1e-4\n",
    "    # LR_MIN = 1e-5\n",
    "    \n",
    "    # Train config\n",
    "    EPOCHS = 10\n",
    "    batch_size = 32\n",
    "    MAX_GRAD_NORM = 2.\n",
    "    GRAD_ACC = 32 // batch_size\n",
    "    check_val_every_n_epoch=1\n",
    "    monitor='val_loss'\n",
    "    monitor_mode='min'\n",
    "    num_warmup_steps=0\n",
    "\n",
    "L.seed_everything(CFG.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f184edb-652e-4450-9ab3-f798297413bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making fold\n",
      "Fold = 0\n",
      "Length of Train = 221, Length of Valid = 56\n",
      "Fold = 1\n",
      "Length of Train = 221, Length of Valid = 56\n",
      "Fold = 2\n",
      "Length of Train = 222, Length of Valid = 55\n",
      "Fold = 3\n",
      "Length of Train = 222, Length of Valid = 55\n",
      "Fold = 4\n",
      "Length of Train = 222, Length of Valid = 55\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/kaggle/input/detect-sleep-states-dataprepare/train_csvs/038441c925bb.csv',\n",
       " '/kaggle/input/detect-sleep-states-dataprepare/train_csvs/03d92c9f6f8a.csv',\n",
       " '/kaggle/input/detect-sleep-states-dataprepare/train_csvs/0402a003dae9.csv',\n",
       " '/kaggle/input/detect-sleep-states-dataprepare/train_csvs/04f547b8017d.csv',\n",
       " '/kaggle/input/detect-sleep-states-dataprepare/train_csvs/062dbd4c95e6.csv']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, StratifiedGroupKFold\n",
    "\n",
    "class MakeKFold():\n",
    "    def __init__(self, n_split=CFG.N_FOLDS, random_state=CFG.SEED, shuffle=True):\n",
    "        self.n_split = n_split\n",
    "        self.random_state = random_state\n",
    "        self.shuffle = shuffle\n",
    "        self.make_fold()\n",
    "\n",
    "    def make_fold(self):\n",
    "        print(\"making fold\")\n",
    "        skf = StratifiedKFold(n_splits=self.n_split, random_state=self.random_state, shuffle=True)\n",
    "        metadata = pd.read_csv('/project/input/child-mind-institute-detect-sleep-states/train_events.csv')\n",
    "        unique_ids = metadata['series_id'].unique()\n",
    "        meta_cts = pd.DataFrame(unique_ids, columns=['series_id'])\n",
    "        \n",
    "        fold_train_ids = []\n",
    "        fold_valid_ids = []\n",
    "        \n",
    "        for i, (train_index, valid_index) in enumerate(skf.split(X=meta_cts['series_id'], y=[1]*len(meta_cts))):\n",
    "            # if i != TRAIN_FOLD:\n",
    "            #     continue\n",
    "            print(f\"Fold = {i}\")\n",
    "            train_ids = meta_cts.loc[train_index, 'series_id']\n",
    "            valid_ids = meta_cts.loc[valid_index, 'series_id']\n",
    "            print(f\"Length of Train = {len(train_ids)}, Length of Valid = {len(valid_ids)}\")\n",
    "            \n",
    "            fold_train_ids.append(train_ids)\n",
    "            fold_valid_ids.append(valid_ids)\n",
    "        self.fold_train_ids = fold_train_ids\n",
    "        self.fold_valid_ids = fold_valid_ids\n",
    "\n",
    "    def train_fpaths(self, fold):\n",
    "        train_fpaths = [f\"/project/input/detect-sleep-states-dataprepare/train_csvs/{_id}.csv\" for _id in self.fold_train_ids[fold]]\n",
    "        return train_fpaths\n",
    "\n",
    "    def valid_fpaths(self, fold):\n",
    "        valid_fpaths = [f\"/project/input/detect-sleep-states-dataprepare/train_csvs/{_id}.csv\" for _id in self.fold_valid_ids[fold]]\n",
    "        return valid_fpaths\n",
    "\n",
    "\n",
    "makefold = MakeKFold()\n",
    "makefold.train_fpaths(0)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fcd6c39-4886-4a11-a65b-54cce305a446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasetを考える\n",
    "#15時区切りと0時区切りを作る。\n",
    "train_df = pl.read_csv('/project/input/child-mind-institute-detect-sleep-states/train_events.csv')\n",
    "#train_df\n",
    "train_df = train_df.with_columns(pl.col('timestamp').str.to_datetime('%Y-%m-%dT%H:%M:%S%z')\n",
    "                                 .dt.hour()\n",
    "                                 .alias(\"datetime_hour\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc490173-f75f-479c-a735-f07072eecc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/kaggle/input/detect-sleep-states-dataprepare/train_csvs/038441c925bb.csv', '/kaggle/input/detect-sleep-states-dataprepare/train_csvs/03d92c9f6f8a.csv', '/kaggle/input/detect-sleep-states-dataprepare/train_csvs/0402a003dae9.csv', '/kaggle/input/detect-sleep-states-dataprepare/train_csvs/04f547b8017d.csv', '/kaggle/input/detect-sleep-states-dataprepare/train_csvs/062dbd4c95e6.csv']\n"
     ]
    }
   ],
   "source": [
    "fold=0\n",
    "print(makefold.train_fpaths(fold)[:5])\n",
    "train_fpathes = makefold.train_fpaths(fold)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "cc08bab0-2227-40ff-852d-ba75b242921a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['step', 'anglez', 'enmo', 'hour', 'onset', 'wakeup']"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "c07128c3-73b6-4257-b89f-fecd82ae0c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>step</th><th>anglez</th><th>enmo</th><th>hour</th><th>onset</th><th>wakeup</th></tr><tr><td>i64</td><td>f64</td><td>f64</td><td>i64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>0</td><td>2.6367</td><td>0.0217</td><td>15</td><td>0.0</td><td>0.0</td></tr><tr><td>1</td><td>2.6368</td><td>0.0215</td><td>15</td><td>0.0</td><td>0.0</td></tr><tr><td>2</td><td>2.637</td><td>0.0216</td><td>15</td><td>0.0</td><td>0.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 6)\n",
       "┌──────┬────────┬────────┬──────┬───────┬────────┐\n",
       "│ step ┆ anglez ┆ enmo   ┆ hour ┆ onset ┆ wakeup │\n",
       "│ ---  ┆ ---    ┆ ---    ┆ ---  ┆ ---   ┆ ---    │\n",
       "│ i64  ┆ f64    ┆ f64    ┆ i64  ┆ f64   ┆ f64    │\n",
       "╞══════╪════════╪════════╪══════╪═══════╪════════╡\n",
       "│ 0    ┆ 2.6367 ┆ 0.0217 ┆ 15   ┆ 0.0   ┆ 0.0    │\n",
       "│ 1    ┆ 2.6368 ┆ 0.0215 ┆ 15   ┆ 0.0   ┆ 0.0    │\n",
       "│ 2    ┆ 2.637  ┆ 0.0216 ┆ 15   ┆ 0.0   ┆ 0.0    │\n",
       "└──────┴────────┴────────┴──────┴───────┴────────┘"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "ef978a6c-2a9a-47d5-8f88-13ccc2477e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>step</th><th>anglez</th><th>enmo</th><th>hour</th><th>onset</th><th>wakeup</th></tr><tr><td>i64</td><td>f64</td><td>f64</td><td>i64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>0</td><td>2.6367</td><td>0.0217</td><td>15</td><td>0.0</td><td>0.0</td></tr><tr><td>1</td><td>2.6368</td><td>0.0215</td><td>15</td><td>0.0</td><td>0.0</td></tr><tr><td>2</td><td>2.637</td><td>0.0216</td><td>15</td><td>0.0</td><td>0.0</td></tr><tr><td>3</td><td>2.6368</td><td>0.0213</td><td>15</td><td>0.0</td><td>0.0</td></tr><tr><td>4</td><td>2.6368</td><td>0.0215</td><td>15</td><td>0.0</td><td>0.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 6)\n",
       "┌──────┬────────┬────────┬──────┬───────┬────────┐\n",
       "│ step ┆ anglez ┆ enmo   ┆ hour ┆ onset ┆ wakeup │\n",
       "│ ---  ┆ ---    ┆ ---    ┆ ---  ┆ ---   ┆ ---    │\n",
       "│ i64  ┆ f64    ┆ f64    ┆ i64  ┆ f64   ┆ f64    │\n",
       "╞══════╪════════╪════════╪══════╪═══════╪════════╡\n",
       "│ 0    ┆ 2.6367 ┆ 0.0217 ┆ 15   ┆ 0.0   ┆ 0.0    │\n",
       "│ 1    ┆ 2.6368 ┆ 0.0215 ┆ 15   ┆ 0.0   ┆ 0.0    │\n",
       "│ 2    ┆ 2.637  ┆ 0.0216 ┆ 15   ┆ 0.0   ┆ 0.0    │\n",
       "│ 3    ┆ 2.6368 ┆ 0.0213 ┆ 15   ┆ 0.0   ┆ 0.0    │\n",
       "│ 4    ┆ 2.6368 ┆ 0.0215 ┆ 15   ┆ 0.0   ┆ 0.0    │\n",
       "└──────┴────────┴────────┴──────┴───────┴────────┘"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pl.read_csv(train_fpathes[0])\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "b557838e-d8d8-4cb6-b079-0125e79d4901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {1:2, 3:4, 5:6}\n",
    "list(d.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "10e59efa-f143-4bd0-90d2-649bb82ec83f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5]"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(d.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6fd8ab-7c2c-47d4-aac2-c205d4c30d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIGMA = 720 #average length of day is 24*60*12 = 17280 for comparison\n",
    "SAMPLE_FREQ = 12 # 1 obs per minute\n",
    "class SleepDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg,\n",
    "        file_pathes,\n",
    "        max_len=2**12,\n",
    "        is_train=False,\n",
    "        sample_per_epoch=10000\n",
    "    ):\n",
    "        self.cfg = cfg\n",
    "        self.enmo_mean = np.load('/project/input/detect-sleep-states-dataprepare/enmo_mean.npy').item()\n",
    "        self.enmo_std = np.load('/project/input/detect-sleep-states-dataprepare/enmo_std.npy').item()\n",
    "        \n",
    "        self.max_len = max_len\n",
    "        \n",
    "        self.is_train = is_train\n",
    "        \n",
    "        self.max_df_size = 0\n",
    "        self.min_df_size = 1e9\n",
    "        \n",
    "        self.sample_per_epoch = sample_per_epoch\n",
    "        \n",
    "        self.feat_list = ['anglez','enmo']\n",
    "        \n",
    "        self.Xys = self.read_csvs(file_pathes)\n",
    "        self.file_pathes = file_pathes\n",
    "        \n",
    "        self.label_list = ['onset', 'wakeup']\n",
    "        \n",
    "        self.hour_feat= ['hour']\n",
    "        \n",
    "    def read_csvs(self, folder):\n",
    "        res = []\n",
    "        features = {}\n",
    "        \n",
    "        if type(folder) is str:\n",
    "            files = glob.glob(f'{folder}/*.csv')\n",
    "        else:\n",
    "            files = folder\n",
    "        for i, f in tqdm(enumerate(files), total=len(files), leave=False):\n",
    "            series_id = os.path.splitext(os.path.basename(self.file_pathes[f]))[0]\n",
    "            df = pl.read_csv(f)\n",
    "            df = self.norm_feat_eng(df, init=True if i==0 else False)\n",
    "\n",
    "            features[f\"{series_id}\"] = df\n",
    "            self.max_df_size = max(self.max_df_size, len(df))\n",
    "            self.min_df_size = min(self.min_df_size, len(df))\n",
    "            #chunk\n",
    "            if not is_train:\n",
    "                num_chunks = (len(df) // cfg.duration) + 1\n",
    "                for i in range(num_chunks):\n",
    "                    chunk_feature = df[i * cfg.duration : (i + 1) * cfg.duration]\n",
    "                    chunk_feature = pad_if_needed(chunk_feature, duration, pad_value=0)\n",
    "                    features[f\"{series_id}_{i:07}\"] = chunk_feature\n",
    "        if is_train:\n",
    "            return res\n",
    "        else:\n",
    "            return features\n",
    "\n",
    "    def pad_if_needed(x: pl.DataFrame, max_len: int, pad_value: float = 0.0) -> np.ndarray:\n",
    "        if len(x) == max_len:\n",
    "            return x\n",
    "        columns = x.columns\n",
    "        num_pad = max_len - len(x)\n",
    "        n_dim = len(x.shape)\n",
    "        pad_widths = [(0, num_pad)] + [(0, 0) for _ in range(n_dim - 1)]\n",
    "        print(pad_widths)\n",
    "        padded = np.pad(x, pad_width=pad_widths, mode=\"constant\", constant_values=pad_value)\n",
    "        output = pl.DataFrame(padded)\n",
    "        output.columns = columns\n",
    "        return output\n",
    "\n",
    "    def norm_feat_eng(self, X, init=False):\n",
    "        X = X.with_columns(pl.col('anglez') / 90)\n",
    "        X = X.with_columns((pl.col('enmo') - self.enmo_mean) / (self.enmo_std + 1e-12))\n",
    "\n",
    "        X = X.fill_nan(0)\n",
    "        \n",
    "        return X.cast(pl.Float32).to_pandas()\n",
    "\n",
    "    def gauss(self,n=SIGMA,sigma=SIGMA*0.15):\n",
    "        # guassian distribution function\n",
    "        r = range(-int(n/2),int(n/2)+1)\n",
    "        return [1 / (sigma * sqrt(2*pi)) * exp(-float(x)**2/(2*sigma**2)) for x in r]\n",
    "    \n",
    "    def __len__(self):\n",
    "        if is_train:\n",
    "            return self.sample_per_epoch if self.is_train else len(self.Xys)\n",
    "        else:\n",
    "            return len(self.features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #filenameの数==datasetの数\n",
    "        if self.is_train:\n",
    "            #augmentation ランダムに時間軸を変える\n",
    "            ind = np.random.randint(0, len(self.Xys))\n",
    "            Xy = self.Xys[ind]\n",
    "            Xy[self.hour_feat] = Xy[self.hour_feat]/24\n",
    "            X = Xy[self.feat_list+self.hour_feat].values.astype(np.float32)\n",
    "            \n",
    "            y = Xy[self.label_list].values.astype(np.float32)\n",
    "\n",
    "            if len(Xy)+1<self.max_len:\n",
    "                res = self.max_len - len(Xy) + 1\n",
    "                X = np.pad(X, ((0, res), (0, 0)))\n",
    "                y = np.pad(y, ((0, res), (0, 0)))\n",
    "\n",
    "            start = np.random.randint(0, len(X)-self.max_len)\n",
    "\n",
    "            X = X[start:start+self.max_len].T\n",
    "            y = y[start:start+self.max_len].T\n",
    "\n",
    "            series_id = os.path.splitext(os.path.basename(self.file_pathes[index]))[0]\n",
    "            return {'series_id':series_id, 'feature':X.T,'label':y.T}\n",
    "\n",
    "        else:\n",
    "            key = list(self.Xys.keys())[index]\n",
    "            Xy = list(self.Xys.values())[index]\n",
    "            Xy[self.hour_feat] = Xy[self.hour_feat]/24\n",
    "            X = Xy[self.feat_list+self.hour_feat].values.astype(np.float32)\n",
    "            y = Xy[self.label_list].values.astype(np.float32)\n",
    "            #t = Xy[self.hour_feat].values.astype(np.int32)\n",
    "        #train shape (3, 8640) label shape (2, 8640)\n",
    "        #channel \n",
    "        #(波形データ、時間データ、ラベル)\n",
    "\n",
    "NOTDEBUG = False\n",
    "\n",
    "fold = 0\n",
    "train_fpaths = makefold.train_fpaths(fold)\n",
    "valid_fpaths = makefold.valid_fpaths(fold)\n",
    "\n",
    "train_fpaths = train_fpaths if NOTDEBUG else train_fpaths[:50]\n",
    "valid_fpaths = valid_fpaths if NOTDEBUG else valid_fpaths[:10]\n",
    "sample_per_epoch = 20_000 if NOTDEBUG else 1_000\n",
    "\n",
    "train_ds = SleepDataset(train_fpaths, max_len=CFG.MAX_LEN, is_train=True, sample_per_epoch=sample_per_epoch)\n",
    "val_ds = SleepDataset(valid_fpaths, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "01fc1b1c-0192-4de6-8330-379540fcb2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "038441c925bb train shape (8640, 3) label shape (8640, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_ds[0]['series_id'],'train shape', train_ds[0]['feature'].shape, 'label shape', train_ds[0]['label'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "90e63203-c60e-477b-9800-0443a404b513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データモジュールを定義\n",
    "from pytorch_lightning import LightningDataModule\n",
    "class LitDataModule(LightningDataModule):\n",
    "    def __init__(self,\n",
    "                 cfg,\n",
    "                 train_fpaths,\n",
    "                 valid_fpaths,\n",
    "                 sample_per_epoch):\n",
    "        super(LitDataModule, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        self.train_fpaths = train_fpaths\n",
    "        self.valid_fpaths = valid_fpaths\n",
    "        self.sample_per_epoch = sample_per_epoch\n",
    "        self.batch_size = cfg.batch_size\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataset = SleepDataset(self.train_fpaths, max_len=CFG.MAX_LEN, is_train=True, sample_per_epoch=self.sample_per_epoch)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=12)\n",
    "        return train_loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        valid_dataset = SleepDataset(self.valid_fpaths, max_len=CFG.MAX_LEN, is_train=True, sample_per_epoch=self.sample_per_epoch)\n",
    "        valid_dataloader = DataLoader(valid_dataset, batch_size=self.batch_size, shuffle=False, num_workers=12)\n",
    "        return valid_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "3d5fce1a-3479-41e9-8d6b-e225107eb1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True, alpha=1., gamma=2.):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        #inputs = F.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        #first compute binary cross-entropy \n",
    "        BCE = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        BCE_EXP = torch.exp(-BCE)\n",
    "        focal_loss = self.alpha * (1-BCE_EXP)**self.gamma * BCE\n",
    "                       \n",
    "        return focal_loss.mean()\n",
    "\n",
    "def add_weight_decay(model, weight_decay=1e-5, skip_list=()):\n",
    "    decay = []\n",
    "    no_decay = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            continue\n",
    "        if len(param.shape) == 1 or np.any([v in name.lower()  for v in skip_list]):\n",
    "            # print(name, 'no decay')\n",
    "            no_decay.append(param)\n",
    "        else:\n",
    "            # print(name, 'decay')\n",
    "            decay.append(param)\n",
    "    return [\n",
    "        {'params': no_decay, 'weight_decay': 0.},\n",
    "        {'params': decay, 'weight_decay': weight_decay}]\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, padding=padding)\n",
    "        self.batchnorm = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.batchnorm(x)\n",
    "        return self.relu(x)\n",
    "\n",
    "class ConvTransposeBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1):\n",
    "        super(ConvTransposeBlock, self).__init__()\n",
    "        self.convtranspose = nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, output_padding=output_padding)\n",
    "        self.batchnorm = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convtranspose(x)\n",
    "        x = self.batchnorm(x)\n",
    "        return self.relu(x)\n",
    "\n",
    "class OreOreUNet1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OreOreUNet1D, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.enc_conv0 = ConvBlock(3, 64)\n",
    "        self.pool0 = nn.MaxPool1d(2)  # Add pooling layers to downsample\n",
    "        self.enc_conv1 = ConvBlock(64, 128)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        self.enc_conv2 = ConvBlock(128, 256)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "        self.enc_conv3 = ConvBlock(256, 512)\n",
    "        \n",
    "        # Decoder\n",
    "        self.dec_conv3 = ConvTransposeBlock(512, 256, stride=2, padding=1, output_padding=1)\n",
    "        self.dec_conv2 = ConvTransposeBlock(256 * 2, 128, stride=2, padding=1, output_padding=1)\n",
    "        self.dec_conv1 = ConvTransposeBlock(128 * 2, 64, stride=2, padding=1, output_padding=1)\n",
    "        self.dec_conv0 = ConvBlock(64 * 2, 2, kernel_size=1, padding=0)  # Adjust the final layer\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        # Encoder pathway\n",
    "        enc0 = self.enc_conv0(x)\n",
    "        enc0p = self.pool0(enc0)\n",
    "        enc1 = self.enc_conv1(enc0p)\n",
    "        enc1p = self.pool1(enc1)\n",
    "        enc2 = self.enc_conv2(enc1p)\n",
    "        enc2p = self.pool2(enc2)\n",
    "        enc3 = self.enc_conv3(enc2p)\n",
    "\n",
    "        # Decoder pathway\n",
    "        dec3 = self.dec_conv3(enc3)\n",
    "        dec2_input = self.crop_and_concat(dec3, enc2)  # Crop enc2 to match dec3 if necessary\n",
    "        dec2 = self.dec_conv2(dec2_input)\n",
    "        dec1_input = self.crop_and_concat(dec2, enc1)  # Crop enc1 to match dec2 if necessary\n",
    "        dec1 = self.dec_conv1(dec1_input)\n",
    "        dec0_input = self.crop_and_concat(dec1, enc0)  # Crop enc0 to match dec1 if necessary\n",
    "        logits = self.dec_conv0(dec0_input)\n",
    "\n",
    "        #output\n",
    "        output = {\"logits\": logits}\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "            output[\"loss\"] = loss\n",
    "\n",
    "        return output\n",
    "\n",
    "    def crop_and_concat(self, upsampled, bypass):\n",
    "        \"\"\" Crop the bypass to the same size as upsampled and concatenate \"\"\"\n",
    "        diff = bypass.size()[2] - upsampled.size()[2]\n",
    "        bypass = F.pad(bypass, (-diff // 2, -diff - (-diff // 2)))\n",
    "        return torch.cat((upsampled, bypass), 1)      \n",
    "\n",
    "from pytorch_lightning import LightningModule\n",
    "# PyTorch Lightningモジュールを定義\n",
    "class LitModule(LightningModule):\n",
    "    def __init__(self, cfg):\n",
    "        super(LitModule, self).__init__()\n",
    "        #cfg.table_feature_num = len(train_ds.feat_list)\n",
    "        self.model = OreOreUNet1D()\n",
    "        self.criterion = FocalLoss(alpha=1., gamma=2.)\n",
    "        self.cfg = cfg\n",
    "        self.duration = cfg.MAX_LEN\n",
    "        self.validation_step_outputs: list = []\n",
    "        self.__best_loss = np.inf\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, labels: Optional[torch.Tensor] = None\n",
    "    ) -> dict[str, Optional[torch.Tensor]]:\n",
    "        return self.model(x, labels)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.__share_step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.__share_step(batch, \"val\")\n",
    "\n",
    "    def __share_step(self, batch, mode: str) -> torch.Tensor:\n",
    "        output = self.model(batch[\"feature\"], batch[\"label\"])\n",
    "        loss: torch.Tensor = output[\"loss\"]\n",
    "        logits = output[\"logits\"]  # (batch_size, n_timesteps, n_classes)\n",
    "\n",
    "        if mode == \"train\":\n",
    "            self.log(\n",
    "                f\"{mode}_loss\",\n",
    "                loss.detach().item(),\n",
    "                on_step=False,\n",
    "                on_epoch=True,\n",
    "                logger=True,\n",
    "                prog_bar=True,\n",
    "            )\n",
    "        elif mode == \"val\":\n",
    "            resized_logits = resize(\n",
    "                logits.sigmoid().detach().cpu(),\n",
    "                size=[self.duration, logits.shape[2]],\n",
    "                antialias=False,\n",
    "            )\n",
    "            resized_labels = resize(\n",
    "                batch[\"label\"].detach().cpu(),\n",
    "                size=[self.duration, logits.shape[2]],\n",
    "                antialias=False,\n",
    "            )\n",
    "            self.validation_step_outputs.append(\n",
    "                (\n",
    "                    batch[\"key\"],\n",
    "                    resized_labels.numpy(),\n",
    "                    resized_logits.numpy(),\n",
    "                    loss.detach().item(),\n",
    "                )\n",
    "            )\n",
    "            self.log(\n",
    "                f\"{mode}_loss\",\n",
    "                loss.detach().item(),\n",
    "                on_step=False,\n",
    "                on_epoch=True,\n",
    "                logger=True,\n",
    "                prog_bar=True,\n",
    "            )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        keys = []\n",
    "        for x in self.validation_step_outputs:\n",
    "            keys.extend(x[0])\n",
    "        labels = np.concatenate([x[1] for x in self.validation_step_outputs])\n",
    "        preds = np.concatenate([x[2] for x in self.validation_step_outputs])\n",
    "        losses = np.array([x[3] for x in self.validation_step_outputs])\n",
    "        loss = losses.mean()\n",
    "\n",
    "        val_pred_df = post_process_for_seg(\n",
    "            keys=keys,\n",
    "            preds=preds[:, :, [1, 2]],\n",
    "            score_th=self.cfg.post_process.score_th,\n",
    "            distance=self.cfg.post_process.distance,\n",
    "        )\n",
    "        score = event_detection_ap(self.val_event_df.to_pandas(), val_pred_df.to_pandas())\n",
    "        self.log(\"val_score\", score, on_step=False, on_epoch=True, logger=True, prog_bar=True)\n",
    "\n",
    "        if loss < self.__best_loss:\n",
    "            np.save(\"keys.npy\", np.array(keys))\n",
    "            np.save(\"labels.npy\", labels)\n",
    "            np.save(\"preds.npy\", preds)\n",
    "            val_pred_df.write_csv(\"val_pred_df.csv\")\n",
    "            torch.save(self.model.state_dict(), \"best_model.pth\")\n",
    "            print(f\"Saved best model {self.__best_loss} -> {loss}\")\n",
    "            self.__best_loss = loss\n",
    "\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        #lr\n",
    "        optimizer_parameters = add_weight_decay(model, weight_decay=self.cfg.WD, skip_list=['bias'])\n",
    "        optimizer = AdamW(optimizer_parameters, lr=self.cfg.LR, eps=1e-6, betas=(0.9, 0.999))\n",
    "        \n",
    "        #shcedular\n",
    "        scheduler = get_cosine_schedule_with_warmup(optimizer,\n",
    "                                                    num_training_steps=self.trainer.max_steps,\n",
    "                                                    num_warmup_steps=self.cfg.num_warmup_steps)\n",
    "\n",
    "        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\"}]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "afc5113e-4859-4044-b800-68a1d55d8a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9499, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor([[[0.0000, 1.1383, 0.0000,  ..., 0.0000, 0.0000, 1.1501],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0022, 0.0000, 0.0000]]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = LitModule(CFG)\n",
    "\n",
    "# Example input tensor of shape (batch_size, channels, length)\n",
    "input_tensor = torch.randn(1, 3, 8640)\n",
    "output_tensor = torch.randn(1, 2, 8640)\n",
    "\n",
    "batch = {'feature':input_tensor, 'label':output_tensor}\n",
    "# Forward pass\n",
    "output_tensor = model(input_tensor, output_tensor)\n",
    "print(output_tensor['loss'])\n",
    "print(output_tensor['logits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "fd63cba1-2bf8-4b46-b714-d2efecb194e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                   \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">    </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name            </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type               </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0  </span>│ model           │ OreOreUNet1D       │  1.2 M │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1  </span>│ model.enc_conv0 │ ConvBlock          │    768 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2  </span>│ model.pool0     │ MaxPool1d          │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3  </span>│ model.enc_conv1 │ ConvBlock          │ 25.0 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4  </span>│ model.pool1     │ MaxPool1d          │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5  </span>│ model.enc_conv2 │ ConvBlock          │ 99.1 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6  </span>│ model.pool2     │ MaxPool1d          │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7  </span>│ model.enc_conv3 │ ConvBlock          │  394 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8  </span>│ model.dec_conv3 │ ConvTransposeBlock │  393 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 9  </span>│ model.dec_conv2 │ ConvTransposeBlock │  196 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 10 </span>│ model.dec_conv1 │ ConvTransposeBlock │ 49.3 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 11 </span>│ model.dec_conv0 │ ConvBlock          │    262 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 12 </span>│ model.loss_fn   │ BCEWithLogitsLoss  │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 13 </span>│ criterion       │ FocalLoss          │      0 │\n",
       "└────┴─────────────────┴────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m  \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName           \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType              \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0 \u001b[0m\u001b[2m \u001b[0m│ model           │ OreOreUNet1D       │  1.2 M │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1 \u001b[0m\u001b[2m \u001b[0m│ model.enc_conv0 │ ConvBlock          │    768 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2 \u001b[0m\u001b[2m \u001b[0m│ model.pool0     │ MaxPool1d          │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3 \u001b[0m\u001b[2m \u001b[0m│ model.enc_conv1 │ ConvBlock          │ 25.0 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4 \u001b[0m\u001b[2m \u001b[0m│ model.pool1     │ MaxPool1d          │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m5 \u001b[0m\u001b[2m \u001b[0m│ model.enc_conv2 │ ConvBlock          │ 99.1 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m6 \u001b[0m\u001b[2m \u001b[0m│ model.pool2     │ MaxPool1d          │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m7 \u001b[0m\u001b[2m \u001b[0m│ model.enc_conv3 │ ConvBlock          │  394 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m8 \u001b[0m\u001b[2m \u001b[0m│ model.dec_conv3 │ ConvTransposeBlock │  393 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m9 \u001b[0m\u001b[2m \u001b[0m│ model.dec_conv2 │ ConvTransposeBlock │  196 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m10\u001b[0m\u001b[2m \u001b[0m│ model.dec_conv1 │ ConvTransposeBlock │ 49.3 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m11\u001b[0m\u001b[2m \u001b[0m│ model.dec_conv0 │ ConvBlock          │    262 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m12\u001b[0m\u001b[2m \u001b[0m│ model.loss_fn   │ BCEWithLogitsLoss  │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m13\u001b[0m\u001b[2m \u001b[0m│ criterion       │ FocalLoss          │      0 │\n",
       "└────┴─────────────────┴────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 1.2 M                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 1.2 M                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 4                                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 1.2 M                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 1.2 M                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 4                                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                   "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa4d03c8a2040c58fff0b84d0df1ed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'key'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[191], line 44\u001b[0m\n\u001b[1;32m     22\u001b[0m model_summary \u001b[38;5;241m=\u001b[39m RichModelSummary(max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     24\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# env\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     default_root_dir\u001b[38;5;241m=\u001b[39mPath\u001b[38;5;241m.\u001b[39mcwd(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m     check_val_every_n_epoch\u001b[38;5;241m=\u001b[39mCFG\u001b[38;5;241m.\u001b[39mcheck_val_every_n_epoch,\n\u001b[1;32m     43\u001b[0m )\n\u001b[0;32m---> 44\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatamodule\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:532\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m    531\u001b[0m _verify_strategy_supports_compile(model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy)\n\u001b[0;32m--> 532\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:43\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     46\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:571\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector\u001b[38;5;241m.\u001b[39mattach_data(\n\u001b[1;32m    562\u001b[0m     model, train_dataloaders\u001b[38;5;241m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[38;5;241m=\u001b[39mval_dataloaders, datamodule\u001b[38;5;241m=\u001b[39mdatamodule\n\u001b[1;32m    563\u001b[0m )\n\u001b[1;32m    565\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    567\u001b[0m     ckpt_path,\n\u001b[1;32m    568\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    569\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    570\u001b[0m )\n\u001b[0;32m--> 571\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:980\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    977\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 980\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1023\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1021\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1023\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:202\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:355\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher\u001b[38;5;241m.\u001b[39msetup(combined_loader)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:134\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance(data_fetcher)\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_advance_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:249\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.on_advance_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_check_val:\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mvalidating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# update plateau LR scheduler after metrics are logged\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:181\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:115\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m     previous_dataloader_idx \u001b[38;5;241m=\u001b[39m dataloader_idx\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:376\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_started()\n\u001b[1;32m    375\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 376\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    380\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_test_batch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_validation_batch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:294\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 294\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    297\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:393\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mval_step_context():\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, ValidationStep)\n\u001b[0;32m--> 393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[190], line 138\u001b[0m, in \u001b[0;36mLitModule.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidation_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__share_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[190], line 167\u001b[0m, in \u001b[0;36mLitModule.__share_step\u001b[0;34m(self, batch, mode)\u001b[0m\n\u001b[1;32m    155\u001b[0m     resized_logits \u001b[38;5;241m=\u001b[39m resize(\n\u001b[1;32m    156\u001b[0m         logits\u001b[38;5;241m.\u001b[39msigmoid()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu(),\n\u001b[1;32m    157\u001b[0m         size\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mduration, logits\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]],\n\u001b[1;32m    158\u001b[0m         antialias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    159\u001b[0m     )\n\u001b[1;32m    160\u001b[0m     resized_labels \u001b[38;5;241m=\u001b[39m resize(\n\u001b[1;32m    161\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu(),\n\u001b[1;32m    162\u001b[0m         size\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mduration, logits\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]],\n\u001b[1;32m    163\u001b[0m         antialias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    164\u001b[0m     )\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_step_outputs\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    166\u001b[0m         (\n\u001b[0;32m--> 167\u001b[0m             \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkey\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m    168\u001b[0m             resized_labels\u001b[38;5;241m.\u001b[39mnumpy(),\n\u001b[1;32m    169\u001b[0m             resized_logits\u001b[38;5;241m.\u001b[39mnumpy(),\n\u001b[1;32m    170\u001b[0m             loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m    171\u001b[0m         )\n\u001b[1;32m    172\u001b[0m     )\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\n\u001b[1;32m    174\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    175\u001b[0m         loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    179\u001b[0m         prog_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    180\u001b[0m     )\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[0;31mKeyError\u001b[0m: 'key'"
     ]
    }
   ],
   "source": [
    "NOTDEBUG = False\n",
    "\n",
    "fold = 0\n",
    "train_fpaths = makefold.train_fpaths(fold)\n",
    "valid_fpaths = makefold.valid_fpaths(fold)\n",
    "\n",
    "train_fpaths = train_fpaths if NOTDEBUG else train_fpaths[:50]\n",
    "valid_fpaths = valid_fpaths if NOTDEBUG else valid_fpaths[:10]\n",
    "sample_per_epoch = 20_000 if NOTDEBUG else 1_000\n",
    "datamodule = LitDataModule(CFG, train_fpaths, valid_fpaths, sample_per_epoch)\n",
    "model = LitModule(CFG)\n",
    "\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    verbose=True,\n",
    "    monitor=CFG.monitor,\n",
    "    mode=CFG.monitor_mode,\n",
    "    save_top_k=1,\n",
    "    save_last=False,\n",
    ")\n",
    "lr_monitor = LearningRateMonitor(\"epoch\")\n",
    "progress_bar = RichProgressBar()\n",
    "model_summary = RichModelSummary(max_depth=2)\n",
    "\n",
    "trainer = Trainer(\n",
    "    # env\n",
    "    default_root_dir=Path.cwd(),\n",
    "    # num_nodes=cfg.training.num_gpus,\n",
    "    # accelerator=cfg.accelerator,\n",
    "    precision=16 if CFG.USE_AMP else 32,\n",
    "    # training\n",
    "    # fast_dev_run=cfg.debug,  # run only 1 train batch and 1 val batch\n",
    "    max_epochs=CFG.EPOCHS,\n",
    "    max_steps=CFG.EPOCHS * len(datamodule.train_dataloader()),\n",
    "    # gradient_clip_val=cfg.gradient_clip_val,\n",
    "    # accumulate_grad_batches=cfg.accumulate_grad_batches,\n",
    "    callbacks=[checkpoint_cb, lr_monitor, progress_bar, model_summary],\n",
    "    # logger=pl_logger,\n",
    "    # resume_from_checkpoint=resume_from,\n",
    "    num_sanity_val_steps=0,\n",
    "    log_every_n_steps=int(len(datamodule.train_dataloader()) * 0.1),\n",
    "    sync_batchnorm=True,\n",
    "    check_val_every_n_epoch=CFG.check_val_every_n_epoch,\n",
    ")\n",
    "trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c23e13-e0f0-45ad-aa37-f3af4b698c69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
